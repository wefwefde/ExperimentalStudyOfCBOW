{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8af29103"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import numpy as np\n","import gensim\n","import gensim.downloader\n","!pip install datasets\n","import datasets\n","from datasets import load_dataset\n","import pandas as pd\n","import torchtext\n","from torchtext.data import get_tokenizer\n","import warnings\n","warnings.filterwarnings('ignore')\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","wiki_data_train = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"train\").shuffle()\n","wiki_data_test = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"test\").shuffle()\n","WIKI_TRAIN = pd.DataFrame(wiki_data_train)\n","WIKI_TEST = pd.DataFrame(wiki_data_test)\n","#WIKI_ALL = pd.concat([WIKI_TRAIN, WIKI_TEST])\n","my_tokenizer = get_tokenizer(\"basic_english\")\n","UNK_token = 0\n","UNK_symbol = '<unk>'\n","class Vocab:\n","  def __init__(self, name=''):\n","    self.name = name\n","    self._word2index = {UNK_symbol: UNK_token}\n","    self._word2count = {UNK_symbol: 0}\n","    self._index2word = {UNK_token: UNK_symbol}\n","    self._n_words = 1\n","  def get_words(self):\n","    return list(self._word2count.keys())\n","  def num_words(self):\n","    return self._n_words\n","  def word2index(self, word):\n","    if word in self._word2index:\n","      return self._word2index[word]\n","    else:\n","      return self._word2index[UNK_symbol]\n","  def index2word(self, word):\n","    return self._index2word[word]\n","  def word2count(self, word):\n","    return self._word2count[word]\n","  def add_sentence(self, sentence):\n","    for word in sentence.split(' '):\n","      self.add_word(word)\n","  def add_word(self, word):\n","    if word not in self._word2index:\n","      self._word2index[word] = self._n_words\n","      self._word2count[word] = 1\n","      self._index2word[self._n_words] = word\n","      self._n_words += 1\n","    else:\n","      self._word2count[word] += 1"],"id":"8af29103"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHE0ebI9yEQG"},"outputs":[],"source":["CBOW_MAX_LENGTH = 400\n","CBOW_WINDOW = 4"],"id":"RHE0ebI9yEQG"},{"cell_type":"code","execution_count":null,"metadata":{"id":"n00-nJdBiiLL"},"outputs":[],"source":["def prep_cbow_data(data_frame, tokenizer_fn, window=2, max_length=50):\n","  data_out = []\n","  data_oo = []\n","  vocab = Vocab()\n","  for i in range(len(data_frame[\"text\"])):\n","    tokens = tokenizer_fn(data_frame[\"text\"][i])\n","    lento = len(tokens)\n","    if lento < max_length:\n","      end = lento\n","    else:\n","      end = max_length\n","    indices = np.zeros(end)\n","    for j in range(lento):\n","      vocab.add_word(tokens[j])\n","    for j in range(end):\n","      indices[j] = vocab.word2index(tokens[j])\n","    for j in range(window,end-window):\n","      x = []\n","      y = 0\n","      for k in range(j-window,j+window+1):\n","        if k != j:\n","          x.append(indices[k])\n","        else:\n","          y = indices[k]\n","      data_out.append((x,y))\n","      x.append(y)\n","      data_oo.append(x)\n","  return data_out, vocab,data_oo"],"id":"n00-nJdBiiLL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RURhzFdLkIT_"},"outputs":[],"source":["CBOW_DATA, CBOW_VOCAB, DATA = prep_cbow_data(WIKI_TRAIN, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)\n","print(\"len dataframe=\", len(WIKI_TRAIN), \"len data=\", len(CBOW_DATA))"],"id":"RURhzFdLkIT_"},{"cell_type":"code","source":["!pip install twinning\n","DATA = np.array(DATA)\n","CBOW_DATA = np.array(CBOW_DATA)"],"metadata":{"id":"YUolY_vzEDKQ"},"id":"YUolY_vzEDKQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from twinning import twin\n","INDEX = twin(DATA,r=2)\n","CBOW_DATA = CBOW_DATA[np.int64(INDEX)]\n","CBOW_DATA = CBOW_DATA.tolist()\n","del DATA\n","del INDEX"],"metadata":{"id":"AldcKO5uEHiJ"},"id":"AldcKO5uEHiJ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OphI3AWSlG5A"},"outputs":[],"source":["def get_batch(data, index, batch_size=10):\n","  dd = list(zip(*data[index*batch_size:(index+1)*batch_size]))\n","  x = np.array(dd[0])\n","  y = np.array(dd[1])\n","  x = torch.tensor(x)\n","  y = torch.tensor(y)\n","  x = x.to(torch.int64)\n","  x = x.to(DEVICE)\n","  y = y.to(torch.int64)\n","  y = y.to(DEVICE)\n","  return x, y\n","class CBOW(nn.Module):\n","  def __init__(self, vocab_size, embed_size):\n","    super(CBOW, self).__init__()\n","    self.emb1 = nn.Embedding(int(vocab_size),int(embed_size))\n","    self.linear2 = nn.Linear(int(embed_size),int(vocab_size))\n","    self.log3 = nn.LogSoftmax()\n","  def forward(self, x):\n","    probs = None\n","    emb1 = self.emb1(x).sum(axis=1)\n","    linear2 = self.linear2(emb1)\n","    probs = self.log3(linear2)\n","    probs = probs.to(torch.float)\n","    return probs\n","def train_cbow(model, data, num_epochs, batch_size, criterion, optimizer):\n","  for epoch in range(int(num_epochs)):\n","    losses = []\n","    for i in range(len(data)//int(batch_size)):\n","      x, y = get_batch(data, i, int(batch_size))\n","      y_hat = model(x)\n","      loss = criterion(y_hat, y)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      losses.append(loss.item())\n","      optimizer.step()\n","      if i % 100 == 0:\n","        print('iter', i, 'loss', np.array(losses).mean())\n","    print('epoch', epoch, 'loss', np.array(losses).mean())"],"id":"OphI3AWSlG5A"},{"cell_type":"code","source":["CBOW_DATA[0:10]"],"metadata":{"id":"yY0ukMUDhguX"},"id":"yY0ukMUDhguX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["WIKI_TRAIN[\"text\"][0:2]"],"metadata":{"id":"PmPbdOecKTxL"},"id":"PmPbdOecKTxL","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QmmG2lY-icYZ"},"outputs":[],"source":["CBOW_EMBED_DIMENSIONS = 70\n","CBOW_BATCH_SIZE = 418\n","CBOW_NUM_EPOCHS = 13\n","CBOW_LEARNING_RATE = 0.00132"],"id":"QmmG2lY-icYZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEja3g8zmxFX"},"outputs":[],"source":["cbow_model = CBOW(CBOW_VOCAB.num_words(), CBOW_EMBED_DIMENSIONS)\n","cbow_model.to(DEVICE)\n","CBOW_CRITERION = nn.NLLLoss()\n","CBOW_OPTIMIZER = torch.optim.AdamW(cbow_model.parameters(), lr=CBOW_LEARNING_RATE)\n","train_cbow(cbow_model, CBOW_DATA, num_epochs=CBOW_NUM_EPOCHS, batch_size=CBOW_BATCH_SIZE, criterion=CBOW_CRITERION, optimizer=CBOW_OPTIMIZER)"],"id":"wEja3g8zmxFX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"zhmcyslOnOG5"},"outputs":[],"source":["def prep_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n","  data_out = []\n","  for row in data_frame['text']:\n","    tokens = tokenizer_fn(row)\n","    token_ids = [vocab.word2index(w) for w in tokens]\n","    if len(token_ids) >= (window*2)+1:\n","      token_ids = token_ids[0:min(len(token_ids), max_length)]\n","      for i in range(window, len(token_ids)-window):\n","        x = token_ids[i-window:i] + token_ids[i+1:i+window+1]\n","        y = token_ids[i]\n","        data_out.append((x, y))\n","  return data_out\n","TEST_DATA = prep_test_data(WIKI_TEST, CBOW_VOCAB, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)\n","def test_cbow_performance(model, data, batch_size):\n","  num_correct = 0.\n","  for i in range(len(data)//batch_size):\n","    x, y = get_batch(data, i, batch_size)\n","    y_hat = model(x)\n","    y_hat = torch.topk(y_hat, 10, dim=1).indices\n","    num_correct += ((y_hat - y.unsqueeze(dim=1)) == 0).any(dim=1).sum()\n","  accuracy = num_correct / (len(data) // batch_size * batch_size)\n","  return accuracy\n","accuracy = test_cbow_performance(cbow_model, TEST_DATA, 512)\n","print(accuracy)"],"id":"zhmcyslOnOG5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"so8FupXkkua9"},"outputs":[],"source":["def test_cbow_performance(model, data, batch_size):\n","    num_correct = 0.\n","    for i in range(len(data)//batch_size):\n","      x, y = get_batch(data, i, batch_size)\n","      y_hat = model(x)\n","      y_hat = torch.topk(y_hat, 10, dim=1).indices\n","      num_correct += ((y_hat - y.unsqueeze(dim=1)) == 0).any(dim=1).sum()\n","    accuracy = num_correct / (len(data) // batch_size * batch_size)\n","    return float(accuracy)\n","def prep_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n","    data_out = []\n","    for row in data_frame['text']:\n","      tokens = tokenizer_fn(row)\n","      token_ids = [vocab.word2index(w) for w in tokens]\n","      if len(token_ids) >= (window*2)+1:\n","        token_ids = token_ids[0:min(len(token_ids), max_length)]\n","        for i in range(window, len(token_ids)-window):\n","          x = token_ids[i-window:i] + token_ids[i+1:i+window+1]\n","          y = token_ids[i]\n","          data_out.append((x, y))\n","    return data_out"],"id":"so8FupXkkua9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gvdfHXfiZtf"},"outputs":[],"source":["!pip install bayesian-optimization"],"id":"5gvdfHXfiZtf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTzSLd8LimoE"},"outputs":[],"source":["def black_box_function(CBOW_EMBED_DIMENSIONS, CBOW_BATCH_SIZE,CBOW_NUM_EPOCHS ,CBOW_LEARNING_RATE):\n","  cbow_model = CBOW(CBOW_VOCAB.num_words(), CBOW_EMBED_DIMENSIONS)\n","  cbow_model.to(DEVICE)\n","  CBOW_CRITERION = nn.NLLLoss()\n","  CBOW_OPTIMIZER = torch.optim.AdamW(cbow_model.parameters(), lr=CBOW_LEARNING_RATE)\n","  train_cbow(cbow_model, CBOW_DATA, num_epochs=CBOW_NUM_EPOCHS, batch_size=CBOW_BATCH_SIZE, criterion=CBOW_CRITERION, optimizer=CBOW_OPTIMIZER)\n","  TEST_DATA = prep_test_data(WIKI_TEST, CBOW_VOCAB, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)\n","  accuracy = test_cbow_performance(cbow_model, TEST_DATA, 512)\n","  return accuracy\n","from bayes_opt import BayesianOptimization\n","pbounds = {'CBOW_EMBED_DIMENSIONS': (50, 200), 'CBOW_BATCH_SIZE': (128, 1024), \"CBOW_NUM_EPOCHS\":(1,15) ,\"CBOW_LEARNING_RATE\":(1e-4,10e-4)}\n","optimizer = BayesianOptimization(\n","    f=black_box_function,\n","    pbounds=pbounds,\n","    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n","    random_state=1,\n",")"],"id":"WTzSLd8LimoE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xQKOuUCi1hS"},"outputs":[],"source":["optimizer.maximize(\n","    init_points=6,\n","    n_iter=200,\n",")"],"id":"5xQKOuUCi1hS"}],"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}